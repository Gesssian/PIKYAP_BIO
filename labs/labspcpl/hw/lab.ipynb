{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a300afb8-7632-4a8a-b870-cecc22d73654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from abc import ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e7aa61-4dd6-4418-84e5-ae773756eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLoss(ABC):\n",
    "    def calc_loss(X:np.ndarray, y:np.ndarray, w:np.ndarray) -> float:\n",
    "        raise NotImplementError\n",
    "    def calc_grad(X:np.ndarray, y:np.ndarray, w:np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a8e982-221e-4a06-ae6f-2a030100f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticLoss(BaseLoss):\n",
    "     def calc_loss(self, X:np.ndarray, y:np.ndarray, w:np.ndarray) -> float:\n",
    "         Q = 0\n",
    "         for i in range(len(y)):\n",
    "             a = 1/(1+np.e**(-np.dot(w,X[i])))\n",
    "             Q += y[i]*np.log(a)+(1-y[i])*np.log(1-a)\n",
    "         return -Q/len(y)\n",
    "     def calc_grad(self, X:np.ndarray, y:np.ndarray, w:np.ndarray) -> np.ndarray:\n",
    "         grad = 0\n",
    "         for i in range(len(y)):\n",
    "             a = 1/(1+np.e**(-np.dot(w,X[i])))\n",
    "             grad += X[i] * (y[i]-a)\n",
    "             return -grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b6b626-e3a6-45ae-bb9d-b8829084f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hinge(BaseLoss):\n",
    "    def calc_loss(X:np.ndarray, y:np.ndarray, w:np.ndarray) -> float:\n",
    "        Q = 0\n",
    "        for i in range(len(y)):\n",
    "             Q += max(0, 1 - y[i]* np.dot(X[i], w))\n",
    "        return -Q/len(y)\n",
    "    def calc_grad(X:np.ndarray, y:np.ndarray, w:np.ndarray) -> np.ndarray:\n",
    "         grad = 0\n",
    "         for i in range(len(y)):\n",
    "             if y[i]*(np.dot(X[i], w)) > 0:\n",
    "                 continue\n",
    "             else:\n",
    "                 grad += y[i]*X\n",
    "         return -grad/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde046fc-97e7-4a33-b957-ff6314cc1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rozenblatt(BaseLoss):\n",
    "    def calc_loss(X:np.ndarray, y:np.ndarray, w:np.ndarray) -> float:\n",
    "        Q = 0\n",
    "        for i in range(len(y)):\n",
    "             Q += max(0, y[i]* np.dot(X[i], w))\n",
    "        return -Q/len(y)\n",
    "    def calc_grad(X:np.ndarray, y:np.ndarray, w:np.ndarray) -> np.ndarray:\n",
    "         grad = 0\n",
    "         for i in range(len(y)):\n",
    "             if y[i]*(np.dot(X[i], w)) > 0:\n",
    "                 continue\n",
    "             else:\n",
    "                 grad += y[i]*X\n",
    "         return -grad/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0618fd-27ec-4085-9970-414f990dc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X: np.ndarray, n_components: int) -> np.ndarray:\n",
    "    mean = np.mean(X, axis=0)\n",
    "    centered_X = X - mean\n",
    "\n",
    "    cov_matrix = np.cov(centered_X.T)\n",
    "\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    top_eigenvectors = eigenvectors[:, sorted_indices[:n_components]]\n",
    "\n",
    "    transformed_X = np.dot(centered_X, top_eigenvectors)\n",
    "\n",
    "    return transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "268de7bb-cad9-497a-a457-fd425d09ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBayesianClassifier:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_priors = {}\n",
    "        self.mean_vectors = {}\n",
    "        self.cov_matrices = {}\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.class_priors[c] = len(X_c) / len(X)\n",
    "            self.mean_vectors[c] = np.mean(X_c, axis=0)\n",
    "            self.cov_matrices[c] = np.cov(X_c, rowvar=False)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            posteriors = []\n",
    "            for c in self.classes:\n",
    "                prior = self.class_priors[c]\n",
    "                mean = self.mean_vectors[c]\n",
    "                cov = self.cov_matrices[c]\n",
    "                likelihood = multivariate_normal(mean=mean, cov=cov).pdf(x)\n",
    "                posterior = prior * likelihood\n",
    "                posteriors.append(posterior)\n",
    "            predictions.append(np.argmax(posteriors))\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae5e9db-d841-4c84-812f-096238e9dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(BaseLoss):\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        Q = ((np.linalg.norm(np.dot(X,w) - y))**2)/len(y)\n",
    "        return Q\n",
    "        \n",
    "        \n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        L = np.dot(X,w) - y\n",
    "        Xt = np.transpose(X)\n",
    "        Grad = 2*np.dot(Xt, L)/len(y)\n",
    "        return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "406c1361-4658-4a61-9b29-965380181475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w_init: np.ndarray, X: np.ndarray, y: np.ndarray,\n",
    "                        loss: BaseLoss, lr: float, n_iterations: int = 100000):\n",
    "    W = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        w_init = w_init - lr*loss.calc_grad(X,y, w_init)\n",
    "        W.append(w_init)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a61a940f-801a-439a-871f-fd9780276a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg1:\n",
    "    def __init__(self, loss: BaseLoss, lr: float = 0.1) -> None:\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        self.w = None\n",
    "        self.g = None\n",
    "\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LogReg':\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        X = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "        shape_X = X.shape\n",
    "\n",
    "        self.w = np.ones(shape_X[-1])\n",
    "        self.g = gradient_descent(self.w, X, y, self.loss, lr=self.lr, n_iterations=100000)\n",
    "        return self.g[-1]\n",
    "        \n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        assert hasattr(self, \"w\"), \"Log regression must be fitted first\"\n",
    "        assert hasattr(self, \"g\"), \"Log regression must be fitted first\"\n",
    "        X = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "        y =[]\n",
    "        for i in range(X.shape[0]):\n",
    "            a = 1/(1+np.e**(-np.dot(self.w,X[i])))\n",
    "            y.append(a)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07dfa71e-7504-4f1d-b083-7e1858e308cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression1:\n",
    "    def __init__(self, loss: BaseLoss, lr: float = 0.1) -> None:\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        self.w = None\n",
    "        self.g = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        X = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "        shape_X = X.shape\n",
    "\n",
    "        self.w = np.arange(1, shape_X[-1] + 1)\n",
    "        self.g = gradient_descent(self.w, X, y, self.loss, lr=self.lr, n_iterations=100000)\n",
    "        return self.g[-1]\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # Проверяем, что регрессия обучена, то есть, что был вызван fit и в нём был установлен атрибут self.w\n",
    "        assert hasattr(self, \"w\"), \"Linear regression must be fitted first\"\n",
    "        assert hasattr(self, \"g\"), \"Linear regression must be fitted first\"\n",
    "\n",
    "        # добавляем столбец из единиц для константного признака\n",
    "        X = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "        y = np.dot(X, self.g[-1])\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27e47460-afd3-4132-8ce3-0a72dd84c836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "n_features = 2\n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "\n",
    "w_true = np.random.normal(size=(n_features, ))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]  \n",
    "y = X.dot(w_true) + np.random.normal(0, 1, (n_objects))\n",
    "w_init = np.random.uniform(-2, 2, (n_features))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "903fc101-5bf3-4539-a836-9e013130d80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534.3924606415659"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linregr = LinearRegression1(MSELoss(), lr=0.01)\n",
    "linregr.fit(X, y)\n",
    "xs = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "MSELoss().calc_loss(xs, linregr.predict(X), linregr.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a63d197f-26c4-4ea8-a0a5-6b7bb198c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=10000, n_features=10, n_informative=5, n_redundant=5,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f05da76-67c2-4125-af48-94165d2ac652",
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "scl.fit(X)\n",
    "X = scl.transform(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d21db9ac-92e8-434e-b350-05cad0d37f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.04497726, 2.0412664 , 0.03116455, 2.15631163, 0.50712277,\n",
       "       1.25600869, 2.18372638, 2.01450397, 0.31518133, 0.80819938,\n",
       "       2.27238737])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LogReg1(LogisticLoss(), 0.1)\n",
    "lreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c2329f8-ead1-4996-a229-7295542164dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.hstack([x_train, np.ones([x_train.shape[0], 1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1155bde-bc11-4e6e-bb05-4b99c05c8f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3893792899414889"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticLoss().calc_loss(xs, lreg.predict(x_train), lreg.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22816244-201b-48f6-ad2d-5be1f9301f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [1, 3], [2, 4]])\n",
    "y_train = np.array([0, 0, 1, 1, 0, 1])\n",
    "\n",
    "classifier = GaussianBayesianClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "X_test = np.array([[1.5, 2.5], [3.5, 4.5]])\n",
    "predictions = classifier.predict(X_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f429c73-23f0-47e1-856d-5f078e83f23c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
